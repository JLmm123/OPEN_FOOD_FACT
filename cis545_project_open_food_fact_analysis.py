# -*- coding: utf-8 -*-
"""CIS545 Project: Open Food Fact Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WgWqEFnOl1rIoI_zfyuYUbl1KPGyraet

#**CIS545 Project: Open Food Fact Analysis**

```
Renyu Liu, Yuxin Meng, Lingpei Luo
Dec 15th, 2022
```

#**Motivation**

In this project, we take comprehensive data about open food fact to develop various predictive ML models based on the notions of regression, random forests, and feedforward neural nets.

The main objective of the project is to better understand the open food fact using unsupervised and supervised learning. We want to see whether or not we can use regression models to predict nutrition scores and use classification models to predict plan-based and sugary products. In addition, we want to use clustering to segment the whole dataset into several groups and analyze intrinsic patterns.

We also explore questions that interest us about nutrition such as how energy, fat and sugar are related to each other and how they categorize different products.

# **Data loading**
"""

# Sklearn and Pandas Setup
import json
import pandas as pd
import numpy as np
import datetime as dt
import re
import os
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import cm
from google.colab import drive
from wordcloud import WordCloud

!pip install swifter

import swifter

!pip install plotly

!apt update

from google.colab import drive
drive.mount('/content/drive')

!pip install kaggle
!mkdir ~/.kaggle
!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/

!!kaggle datasets download -d openfoodfacts/world-food-facts

!unzip /content/world-food-facts.zip

'''food_df = pd.DataFrame()

#read file chunk by chunk to avoid crash due to RAM
reader = pd.read_csv('/content/en.openfoodfacts.org.products.tsv', sep='\t', chunksize=60000)

for df in reader:
    food_df = food_df.append(df, ignore_index=True)'''

food_df = pd.read_csv('/content/en.openfoodfacts.org.products.tsv', sep='\t')

food_df.shape

"""# **Data preprocessing**

## Check missing values
"""

def missing_value_plot(data, threshold):
    
    plt.figure(figsize=(15,5))
    data_perct=(data.isnull().mean())*100
    data_perct.sort_values(ascending=False).plot.bar(color = 'black', edgecolor = 'black')
    plt.axhline(y = threshold, color='r')
    plt.title('Missing values percentage per feature', fontsize=15, weight='bold' )
    plt.text(len(data.isnull().sum()/len(data))/1.7, threshold+12.5, 'Features with more than %s%s missing values' %(threshold, '%'), fontsize=15,weight='bold', color='red',
         ha='left' ,va='top')
    plt.text(len(data.isnull().sum()/len(data))/1.7, threshold - 5, 'Features with less than %s%s missing values' %(threshold, '%'), fontsize=15,weight='bold', color='blue',
         ha='left' ,va='top')
    plt.xlabel('Features', size=15, weight='bold')
    plt.ylabel('Missing values percentage', weight='bold')
    plt.yticks(weight ='bold')
    
    return plt.show()

missing_value_plot(food_df,90)

food_df_nan = food_df.isnull().sum()
food_df_nan = food_df_nan / food_df.shape[0] * 100

def split_data(threshold):
    cols_of_interest = food_df_nan[food_df_nan <= threshold].index
    data = food_df[cols_of_interest]
    data_1 = data.copy()
    return data_1

food_df_without_nan = split_data(10)

print("Original number of features: " + str(food_df.shape[1]))
print("Number of features with less than 10 % nans: " + str(food_df_without_nan.shape[1]))

"""## Check the rows with non-nan values

We can see that the data have many nan values. All the rows contain at least one nan value. 
"""

# Cleaning if any values are empty in a row
food_df["isempty"] = np.where(food_df[food_df.columns].isnull().sum(axis=1) >= 1, 1, 0)
# Estimation of cleaning proportion
percentage = (food_df.isempty.value_counts()[1] / food_df.shape[0]) * 100
print("Percentage of incomplete tables: " + str(percentage))
print(food_df.isempty.value_counts())

"""## Drop Empty rows"""

food_df_clean = food_df.dropna(axis = 0, how = 'all')

"""## Drop empty columns"""

# find empty rows and columns in the df
def find_empty_columns(df):
    df_missing_columns = df.isnull().sum(axis=0).reset_index()
    df_missing_columns.columns = ['name', 'count']
    df_missing_columns['ratio'] = (df.shape[0]-df_missing_columns['count']) / df.shape[0]
    return df_missing_columns

# there are 16 columns that have no value
df_missing_columns = find_empty_columns(food_df_clean)
print(df_missing_columns[df_missing_columns['ratio'] == 0])

# drop the columns that have no value 
food_df_clean = food_df_clean.dropna(axis = 1, how = 'all')
food_df_clean.shape

"""## Drop row duplicate"""

#TO-DO: Drop the duplicate rows
food_df_clean.drop_duplicates(inplace = True)

"""## Drop outliers"""

print(food_df_clean.shape)

quantile=0.01 # here we set the threshold

num_col = food_df_clean.select_dtypes("float").columns.tolist()
del num_col[-1]

qmax=food_df_clean[num_col].quantile(q=1-quantile, axis=0, interpolation='higher') # we get the 99,9% quantile 
print(qmax)

qmin=food_df_clean[num_col].quantile(q=quantile, axis=0, interpolation='lower') # we get the 0,01% quantile
print(qmin)

for col in num_col:
    valmax=qmax[col]
    valmin=qmin[col]
    
    if (valmax > valmin):
      _clean=food_df_clean[food_df_clean[col]<=valmax] # we keep values under the quantile max
      _clean=food_df_clean[food_df_clean[col]>=valmin] # we keep values over the quantile min
      
print(_clean.shape)

"""We have only **252389 rows** left!"""

food_df_clean = _clean
print(food_df_clean.shape)

"""## Tidying Up

Upon inspection, we can see that the `countries` and `countries_tags` columns are not going to be useful since they have more error and `countries_en` already contains the information. 

We will keep everything else for now and do more data cleaning before we do specific modelling.
"""

# Drop the "countries" and "countries_tags" columns, but keep everything else for now
# we will do more data cleaning before we do specific modelling
food_df_clean = food_df_clean.drop(columns = ['countries','countries_tags'])

"""#**EDA**

##Overview
"""

food_df_clean.shape

"""There are 356027 rows, 146 columns in total. Take an extra look at to columns:"""

food_df_clean.dtypes

food_df_clean.head(5)

food_df_clean.describe()

"""---


Here, for a better understanding of the relationships between the features of the dataset and a better analysis of it, we opted to study the core systematics and meanings of the contents columns of the dataframe.

---
**Hierarchy of contents columns**


---

* fat
 * saturated-fat
   * -butyric-acid
   * -caproic-acid
   * -caprylic-acid
   * -capric-acid
   * -lauric-acid
   * -myristic-acid
   * -palmitic-acid
   * -stearic-acid
   * -arachidic-acid
   * -behenic-acid
   * -lignoceric-acid
   * -cerotic-acid
   * -montanic-acid
   * -melissic-acid
 * monounsaturated-fat
 * polyunsaturated-fat
 * omega-3-fat
   * -alpha-linolenic-acid
   * -eicosapentaenoic-acid
   * -docosahexaenoic-acid
 * omega-6-fat
   * -linoleic-acid
   * -arachidonic-acid
   * -gamma-linolenic-acid
   * -dihomo-gamma-linolenic-acid
 * omega-9-fat
   * -oleic-acid
   * -elaidic-acid
   * -gondoic-acid
   * -mead-acid
   * -erucic-acid
   * -nervonic-acid
 * trans-fat

* cholesterol

* carbohydrates
 * sugars
   * -sucrose
   * -glucose
   * -fructose
   * -lactose
   * -maltose
   * -maltodextrins
 * starch
 * polyols
 * fiber

* proteins
 * casein
 * serum-proteins

* nucleotides

* salt

* sodium

* alcohol

* (vitamins)
 * vitamin-a
 * beta-carotene (provitamin, precursor of vitamin a)
 * vitamin-d
 * vitamin-e
 * vitamin-k
 * vitamin-c
 * vitamin-b1
 * vitamin-b2
 * vitamin-pp
 * vitamin-b6
 * vitamin-b9
 * folates (natural form of vitamin-b9)
 * vitamin-b12
 * biotin (vitamin-b7)
 * pantothenic-acid (vitamin-b5)

* silica

* bicarbonate

* potassium

* chloride

* calcium

* phosphorus

* iron

* magnesium

* zinc

* copper

* manganese

* fluoride

* selenium

* chromium

* molybdenum

* iodine

* caffeine

* taurine

* cocoa

* chlorophyl

##Correlation Matrix

Check how nutrition-score is correlated with other columns:
"""

corr_matrix = food_df_clean.corr()

corr_matrix

#a simple visualization, which is ugly though
sns.heatmap(corr_matrix, cmap='RdBu', vmin=-1.0, vmax=1.0)
plt.title("Correlation Matrix")
plt.show()

"""A better view:"""

# Heatmap for the features
sns.set(context="paper", font_scale = 1.2)
# size of the plot
f, ax = plt.subplots(figsize=(12, 12))
# set the plot heading
f.text(0.45, 0.93, "Correlation Matrix for Nutrition", ha='center', fontsize = 18)
# plot matrix as a heatmap
sns.heatmap(corr_matrix, square=True, linewidths=0.01, cmap="RdBu")
plt.tight_layout()

"""## Relations between selected features and boxplot overview

In this section, we would want to see a boxplot overview for selected features, so as to better understand them before we move onto modelling. These features are:  'saturated-fat_100g', 'fat_100g', 'carbohydrates_100g', 'sugars_100g', 'proteins_100g', 'salt_100g', 'energy_100g'. These seven factors are main branches in our structure, food_df_clean, and are also widely considered as nutrition indicators.
"""

feature_food_df = food_df_clean[['saturated-fat_100g','fat_100g','carbohydrates_100g','sugars_100g','proteins_100g','salt_100g','energy_100g']]

#fill all the Nan value with the mean of this feature
feature_food_df['saturated-fat_100g'].fillna(feature_food_df['saturated-fat_100g'].mean(), inplace = True)
feature_food_df['fat_100g'].fillna(feature_food_df['fat_100g'].mean(), inplace = True)
feature_food_df['carbohydrates_100g'].fillna(feature_food_df['carbohydrates_100g'].mean(), inplace = True)
feature_food_df['sugars_100g'].fillna(feature_food_df['sugars_100g'].mean(), inplace = True)
feature_food_df['proteins_100g'].fillna(feature_food_df['proteins_100g'].mean(), inplace = True)
feature_food_df['salt_100g'].fillna(feature_food_df['salt_100g'].mean(), inplace = True)
feature_food_df['energy_100g'].fillna(feature_food_df['energy_100g'].mean(), inplace = True)

feature_food_df

"""First, let us observe the performance of outliers in our data by assigning red circles for all the outliers."""

red_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white')

fig, axs = plt.subplots(1, len(feature_food_df.columns), figsize=(20,10))

for i, ax in enumerate(axs.flat):
    ax.boxplot(feature_food_df.iloc[:,i], flierprops=red_circle)
    feature_food_df.iloc[:,i]
    ax.set_title(feature_food_df.columns[i], fontsize=20, fontweight='bold')
    ax.tick_params(axis='y', labelsize=14)
    
    #Checking if column names are equal to columns we expect to be logarithmic
    if feature_food_df.columns[i] == 'RDEP' or feature_food_df.columns[i] == 'RMED':
        ax.semilogy()
    
plt.tight_layout()

"""Despite we dropped outliers, we still find most of the features in our data is highly subject to far-away outliers. Next, let us plot without outliers to see their performance."""

fig, axs = plt.subplots(1, len(feature_food_df.columns), figsize=(20,10))

for i, ax in enumerate(axs.flat):
    ax.boxplot(feature_food_df.iloc[:,i], showfliers=False)
    feature_food_df.iloc[:,i]
    ax.set_title(feature_food_df.columns[i], fontsize=20, fontweight='bold')
    ax.tick_params(axis='y', labelsize=14)
    
    #Checking if column names are equal to columns we expect to be logarithmic
    if feature_food_df.columns[i] == 'RDEP' or feature_food_df.columns[i] == 'RMED':
        ax.semilogy()
    
plt.tight_layout()

"""After removing the outliers, we notice that each features is clearly mapped onto various scale. So that scaling may be recommend in the later process.

Next, we take a closer look at the correlation between these seven factors.
"""

corr_seven_matrix = food_df_clean[['saturated-fat_100g','fat_100g','carbohydrates_100g','sugars_100g','proteins_100g','salt_100g','energy_100g']].corr()
#because we have fill Nan value feature_food_df with mean, we use data from food_df_clean
sns.heatmap(corr_seven_matrix, cmap='RdBu', vmin=-1.0, vmax=1.0)
plt.title("Correlation Matrix")
plt.show()

"""From the heatmap, we find high correlation between (fat_100g, saturated-fat_100g), (carbohydrates_100g, sugars_100g). Let us plot some regplots for every pair of them."""

sns.regplot(x="fat_100g", y="saturated-fat_100g", data=food_df_clean)

sns.regplot(x="sugars_100g", y="carbohydrates_100g", data=food_df_clean)

"""We do observe the existence of a linear relationship, but maybe not so strong. To have more data for our models later, we shall leave them to data preprocessing (such as PCA) later before running our models.

## Relations among Proteins, Carbohydrates, and Fat

The characteristics of a food can be reflected by its composition, especially by its contents of protein, carbohydrate and fat. For example, meat contains more protein than other foods but less carbohydrate and fat. Thus, the relations among these key compositions are worth deep inverstigation and we intended to investigate how proteins relate to other components as well, and how their relationships are interconnected.

To analyze the interrelations between pairs, we picked pairplot as it summarizes large amounts of data and clearly visualizes pair relations in a single figure. We then selected 1000 random instances for this part of analysis:
"""

plot = sns.pairplot(food_df_clean.sample(1000), diag_kind='kde', kind='scatter', palette='hls', 
             vars=['proteins_100g', 'carbohydrates_100g', 'fat_100g'], corner=True)
plot.map_lower(sns.kdeplot, levels=4, color=".2")
plot.fig.suptitle('Pairplot of Proteins and Carbohydrates and Fat');

"""---
**Basic Distribution**

To begin with, it is not difficult to see that the products in the dataset generally have a protein content ranging from 0g/100g to 25g/100g, which is a relatively small range, and a fat content less than 40g/100g.

---
**Correlations**

1. When protein content increases, there is a downward trend in the distribution of carbohydrates based on protein content.

2. There is a fairly even distribution of fat based on the level of protein. It is worth noting that there are some outliers, though, in that their fat content is extremely high while their protein content is almost zero. Our speculation is that they might be one of some special products which accounts for a small partition of all products.
"""

food_df_clean[(food_df_clean['fat_100g']>=75) & (food_df_clean['proteins_100g']<=5)]['product_name']

#get a flattened list of product names
product_name_list = food_df_clean[(food_df_clean['fat_100g']>=75) & (food_df_clean['proteins_100g']<=5)]['product_name'].tolist()
product_names = [str(name) for name in product_name_list]
#create a wordcloud
wordcloud = WordCloud(width = 1000, height = 500, background_color ='white').generate(' '.join(product_names))
#plot
plt.figure(figsize=(15,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.title("High-Fat Low-Protein Outliers")
plt.show()

"""A quick glance at this group of products will reveal that Oil makes up the majority of the group of products, along with Mayonnaise, Butter, various varieties of Spread, and a variety of other products.

Similar "outliers" are also found in the pairplot of Fat and Carbohydrates. We wonder if this is the same group of products that were found as "outliers" in the pairplot of fats and proteins that we analyzed earlier.
"""

food_df_clean[(food_df_clean['fat_100g']>=75) & (food_df_clean['carbohydrates_100g']<=5)]['product_name']

#get a flattened list of product names
product_name_list = food_df_clean[(food_df_clean['fat_100g']>=75) & (food_df_clean['carbohydrates_100g']<=5)]['product_name'].tolist()
product_names = [str(name) for name in product_name_list]
#create a wordcloud
wordcloud = WordCloud(width = 1000, height = 500, background_color ='white').generate(' '.join(product_names))
#plot
plt.figure(figsize=(15,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.title("High-Fat Low-Carbohydrates Outliers")
plt.show()

"""It is not suprising to see Oil again being the majority, accompanied by Mayonnaise, Butter, different types, and some other types of food products.

3. The distribution of fat does not show a steep downward trend as the carbohydrate content rises. There is an interesting gap in the distribution of fat contents within the section where the carbohydrate contents range between 20-30%. It appears that few food products with 20-30% carbohydrates contain 20-40% fat, and this scarcity intrigued us.
"""

food_df_clean[(food_df_clean['carbohydrates_100g']>=20) & (food_df_clean['carbohydrates_100g']<=30) & (food_df_clean['fat_100g']<40) & (food_df_clean['fat_100g']>20)]['product_name']

#get a flattened list of product names
product_name_list = food_df_clean[(food_df_clean['carbohydrates_100g']>=20) & (food_df_clean['carbohydrates_100g']<=30) & (food_df_clean['fat_100g']<40) & (food_df_clean['fat_100g']>20)]['product_name'].tolist()
product_names = [str(name) for name in product_name_list]
#create a wordcloud
wordcloud = WordCloud(width = 1000, height = 500, background_color ='white').generate(' '.join(product_names))
#plot
plt.figure(figsize=(15,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.title("Scarce Area")
plt.show()

"""Unfortunately, we failed to find significant commonalities shared by these products which could explain their scarcity: they certainly share some features, such as the fact that they seem all to have high calories, but it is not clear that these characteristics have a meaningful correlation with the scarcity of products with that particular range of fat and carbohydrate contents. 

As a result, we decided not to move further on this path during this project. It could be meaningful to further explore the possible reasons lying behind this odd phenomenon though.

## Check nutrition-score

Check the instance which has null value for nutrition-score.
"""

food_df_clean[food_df_clean['nutrition-score-uk_100g'].isna() == True]
#101171 columns is Nan

#selecting the values which is highly correlated to nutrition-score-uk_100g.
# defined as < -0.8 or > 0.8

correlated_df = corr_matrix[corr_matrix['nutrition-score-uk_100g'].apply(lambda x: x > 0.8 or x < -0.8)]
correlated_df

correlated_df[correlated_df.index =='silica_100g']

"""We noticed that there's significant correlation between nutrition score and silica_100g and lauric acid content.

**In particular, '-lauric-acid_100g' is extremely correlated with nutrition-score.**
"""

correlated_df[correlated_df.index == '-lauric-acid_100g']

sns.regplot(x="-lauric-acid_100g", y="nutrition-score-uk_100g", data=food_df_clean) #we observe a strict linear relation

sns.regplot(x="silica_100g", y="nutrition-score-uk_100g", data=food_df_clean) #we observe a strict linear relation

"""## Countries Information

As shown below, we can see that most products in the dataset are from US and France. So we set our focus on these two countries for many of the rest parts of within this project.
"""

# We will use 'countries_en' column to see countries
countries_top_8_df = food_df_clean['countries_en'].value_counts().head(8).to_frame()
countries_top_8 = countries_top_8_df.style.background_gradient(cmap='Reds')
countries_top_8

# show that US and France have more data
# the difference is significant
food_df_clean['countries_en'].value_counts()[:10][::-1].plot.barh()

"""Here we can tell that products from United States composes a major part of this dataset. We hence set our focus on US products.

## World Food Product Categories

From the wordcloud below, we noticed some prominent keyword, including "plant-based" and "sugar", which we decided to further explore.
"""

def count_words(df, colonne = 'categories_en'):
    list_words = set()
    for word in df[colonne].str.split(','):
        if isinstance(word, float): continue
        list_words = set().union(word, list_words)       
    return list(list_words)

category_keys = count_words(food_df_clean, 'categories_en')

count_keyword = dict()
for index, col in food_df_clean['categories_en'].iteritems():
    if isinstance(col, float): continue
    for s in col.split(','):
        if s in count_keyword.keys():
            count_keyword[s] += 1
        else:
            count_keyword[s] = 1

keyword_census = []
for k,v in count_keyword.items():
    keyword_census.append([k,v])
keyword_census.sort(key = lambda x:x[1], reverse = True)

from wordcloud import WordCloud

words = dict()
trunc_occurences = keyword_census[0:100]
for s in trunc_occurences:
    words[s[0]] = s[1]

wordcloud = WordCloud(width=900,height=500, background_color='black', 
                      max_words=95,
                      color_func = lambda *args, **kwargs: "white")
wordcloud.generate_from_frequencies(words)

plt.figure(figsize=(15,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.title("Food Product Categories")
plt.show()

"""##US Product Names Overview

In this part, we are interested in the product names and want to get an overall impression of them. We limit our scope to products of United States, for the sake of reading convenience.
"""

#select US products
us_food_names = food_df_clean[food_df_clean['countries_en']=='United States']
us_food_names = us_food_names[['product_name', 'categories_en']].dropna(subset=['categories_en'])

"""For the purpose of generating wordclouds, we import relevant libraries and create the function to tokenize test contents for future use."""

import nltk
nltk.download('punkt')

from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords = set(stopwords.words('english'))

from nltk.tokenize import word_tokenize

#function to tokenize text
def tokenize(content):
  alpha_only = []
  if not content:
      print('The text to be tokenized is a None type. Defaulting to blank string.')
      return alpha_only
  tokens = word_tokenize(str(content))
  for token in tokens:
    if token.isalpha() and token.lower() not in stopwords:
      alpha_only.append(token.lower())
  return alpha_only

"""###All Categories

We first examined all the products in United States.
"""

#get a flattened list of product names
product_name_list = us_food_names['product_name'].tolist()
product_names = [str(name) for name in product_name_list]

from wordcloud import WordCloud

#create a wordcloud
wordcloud = WordCloud(width = 1000, height = 500, background_color ='white').generate(' '.join(product_names))
#plot
plt.figure(figsize=(15,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.title("US Product Names")
plt.show()

"""Frequent product names can be noticed from the above figure pretty intuitively. However, we wanted to get some more accurate information about them. Thus, we looked for statistical data and counted the number of appearences for each word."""

#tokenize:
us_food_names['tokenized_product_name'] = us_food_names['product_name'].apply(tokenize)

#flatten:
tokens_lists = us_food_names['tokenized_product_name'].tolist()
tokens = [token for sublist in tokens_lists for token in sublist]

#check most frequent words that appear in product names
from collections import Counter
hot_words_dict = Counter(tokens)
hot_words_df = pd.DataFrame(hot_words_dict, index=[0]).T #cast to dataframe
hot_words_df = hot_words_df.rename({0:'count'}, axis=1) #rename the column
hot_words_df = hot_words_df.sort_values(by=['count'], ascending=False)  #sort in descending order of count
hot_words = hot_words_df.head(5).style.background_gradient(cmap='Reds')  #get top 10 words
hot_words

"""We can infer from the analysis retults that food products listed below are most common in US:

1. Ketchup
2. Juice
3. Tomato, foods containing tomato or tomato-flavored
4. Popcorn
5. Microwaved foods

###Plant-based Category

According to the results in World Food Product Categories part, we have noticed the significance of plant-based category among the whole dataset. Thus in this part, we did extra analysis on plant-based products.
"""

#select products with description "plant-based" in their categories
us_plantbased_food = us_food_names[us_food_names['categories_en'].apply(lambda x: True if ("Plant-based" in x) else False)]

#get a flattened list of product names
product_name_list = us_plantbased_food['product_name'].tolist()
product_names = [str(name) for name in product_name_list]

#create a wordcloud
wordcloud = WordCloud(width = 1000, height = 500, background_color ='white').generate(' '.join(product_names))
#plot
plt.figure(figsize=(15,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.title("US Plant-based Product Names")
plt.show()

"""Again, we sought more specific statistical data so we counted the number of appearences for each word."""

#tokenize:
us_plantbased_food['tokenized_product_name'] = us_plantbased_food['product_name'].apply(tokenize)

#flatten:
tokens_lists = us_plantbased_food['tokenized_product_name'].tolist()
tokens = [token for sublist in tokens_lists for token in sublist]

#check most frequent words that appear in product names
from collections import Counter
hot_words_dict = Counter(tokens)
hot_words_df = pd.DataFrame(hot_words_dict, index=[0]).T #cast to dataframe
hot_words_df = hot_words_df.rename({0:'count'}, axis=1) #rename the column
hot_words_df = hot_words_df.sort_values(by=['count'], ascending=False)  #sort in descending order of count
hot_words = hot_words_df.head(5).style.background_gradient(cmap='Greens')  #get top 10 words
hot_words

"""We can infer from the above analysis that keywords listed below are most common in US plant-based foods:

1. Juice
2. Apple
3. Hamburger, hamburger ingredients, or hamberger accompaniments
4. Buns
5. Aloe, foods containing aloe or aloe-flavored

which are slightly different with the all-category products as a whole.

###Sugary Foods Category
According to the results in World Food Product Categories part, we have noticed the prominence sugary foods among the whole dataset. Thus in this part, we did extra analysis on sugary products.
"""

#select products with description "Sugary" in their categories
us_sugary_food = us_food_names[us_food_names['categories_en'].apply(lambda x: True if ("Sugary" in x) else False)]

#get a flattened list of product names
product_name_list = us_sugary_food['product_name'].tolist()
product_names = [str(name) for name in product_name_list]

#create a wordcloud
wordcloud = WordCloud(width = 1000, height = 500, background_color ='white').generate(' '.join(product_names))
#plot
plt.figure(figsize=(15,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.title("US Sugary Product Names")
plt.show()

#tokenize:
us_sugary_food['tokenized_product_name'] = us_sugary_food['product_name'].apply(tokenize)

#flatten:
tokens_lists = us_sugary_food['tokenized_product_name'].tolist()
tokens = [token for sublist in tokens_lists for token in sublist]

#check most frequent words that appear in product names
from collections import Counter
hot_words_dict = Counter(tokens)
hot_words_df = pd.DataFrame(hot_words_dict, index=[0]).T #cast to dataframe
hot_words_df = hot_words_df.rename({0:'count'}, axis=1) #rename the column
hot_words_df = hot_words_df.sort_values(by=['count'], ascending=False)  #sort in descending order of count
hot_words = hot_words_df.head(5).style.background_gradient(cmap='Blues')  #get top 10 words
hot_words

"""As shown above, we can see that popular sugary products in US are popcorn, butter and chocolate.

# **Unsupervised Machine Learning - Clustering**

## K-means for three factors: Energy, Fat and Sugars

In this section, we want to use the unsupervised machine learning method that we acquire from class to observe how the data is naturally clustered. We will include clustering methods such as k-means, gaussian mixture models, as well as hierarchical methods followed by a short analysis of each.

In the first part, we will use k-means clustering, which is thoroughly expained in our class. Essentially, We will include seven features, 'saturated-fat_100g','fat_100g','carbohydrates_100g','sugars_100g','proteins_100g','salt_100g','energy_100g' to observe how the products can be clustered with these seven well-recognized food indicators.

First of all, we will create functions of create_features to prepoccess the dataframe, then we use run_pipeline to actually run k-means. After that, we will use the method of elbow to find the optimal number for k (the number of clusters). Finally, this part will end with visualizion of wordCloud designed for every clusters in order for readers to comprehend the concepts visually.
"""

from numpy.random.mtrand import normal
from sklearn import preprocessing
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

def create_features(df, unname, return_scaler):
    df = df[[
        'product_name','saturated-fat_100g','fat_100g','carbohydrates_100g','sugars_100g','proteins_100g','salt_100g','energy_100g' # or something else
        ]]
  
    df.dropna(how = 'all', axis = 0, inplace=True)
    df.fillna(0, inplace = True)
    names = df.product_name.tolist()
    df = df.drop(['product_name'], axis=1)
    x = df.values # numpy array
    scaler = preprocessing.StandardScaler()

    x_scaled = scaler.fit_transform(x)
    if unname:
      initial_len = len(names)
      names = []
      for i in range(initial_len):
        names.append(" ")

    if return_scaler:
      return names, pd.DataFrame(x_scaled), scaler

    return names, pd.DataFrame(x_scaled)




def run_pipeline(k, names, X_norm, pca_component):
    pca = PCA(n_components = 3, random_state = 0) # 3D PCA for the plot
    reduced = pd.DataFrame(pca.fit_transform(X_norm))

    kmeans = KMeans(n_clusters=k, random_state = 0)
    
    # Make this student todos 
    # fit the input data
    kmeans = kmeans.fit(reduced)
    # get the cluster labels
    labels = kmeans.predict(reduced)
    # centroid values
    centroid = kmeans.cluster_centers_
    # cluster values
    clusters = kmeans.labels_.tolist()

    reduced['cluster'] = clusters
    reduced['name'] = names
    reduced.columns = ['x', 'y', 'z', 'cluster', 'name']


    if(pca_component):
      return reduced, pca.components_

    return reduced

names, X_norm = create_features(food_df_clean, unname=False, return_scaler = False)
# first, we choose our cluster number to be 15 (we will later re-choose another cluster number)
k = 15
km = run_pipeline(k, names, X_norm, False)
# do a groupby to see how our clusters perform
km.groupby('cluster').count().sort_values('x', ascending = False)

km[km['cluster'] ==3 ]

km[km['cluster'] ==6 ]

"""From the previous table, we can see that if we aggressively set cluster number (i.e. k) to be 15. There will be some outliers in group 3, and group 6. We will drop these data. """

food_df_clustering = food_df_clean.drop(food_df_clean[food_df_clean.product_name.apply(lambda x: x in {'Nusco, Chocolate Spread, Chocolate', 'Prickly Syrup'})].index)

"""### Choosing cluster number

There is a better approach to decide the cluster number than choosing them manually, which is the elbow method. In order to use the notion of elbow method, we consider the notion of distortion, which is simply the sum of mean squared error of each data case to it assigned centroid. The distortion serves as a criteria to decide a better fit. The smaller the distortion is, the better fit our model will be. Though, as the clustors number increases, the distortion will corresponding decreases due to its nature. It is the elbow part (the part where distortion smoothes its decreasing rate with regard to the increase of clustor number)that we will decide on our final cluster data.
"""

def deciding_k(K, names, X_norm):
    pca = PCA(n_components = 3, random_state = 0) # 2D PCA for the plot
    reduced = pd.DataFrame(pca.fit_transform(X_norm))
  

    #compute the distortion
    Sum_of_squared_distances = []

    for k in K:
      kmeans = KMeans(n_clusters=k, max_iter = 100, random_state = 0)  
     
      # fit the input data
      kmeans = kmeans.fit(reduced)
      # get the cluster labels
      labels = kmeans.predict(reduced)
      # centroid values
      centroid = kmeans.cluster_centers_
      # cluster values
      clusters = kmeans.labels_.tolist()
      Sum_of_squared_distances.append(kmeans.inertia_)

    return Sum_of_squared_distances

K = range(1,15)
# notice that we are using food_df_clustering, which already has outliers being cleaned
names, X_norm = create_features(food_df_clustering, unname=False, return_scaler= False)
distortion = deciding_k(K, names, X_norm)

plt.plot(K, distortion, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum_of_squared_distances')
plt.title('Elbow Method For Optimal k')
plt.show()

"""As we saw from the graph, the area of where 8, 9 or 10 located is exactly an "elbow", hence a greater cluster number. In this way, we can manually pick a number for our choice. And 8 will be the lucky number ✋. """

# 8 is our chosen cluster number 
k = 8
names, X_norm, scaler_returned= create_features(food_df_clustering, unname = False, return_scaler = True)
km, pca_component_analysis = run_pipeline(k, names, X_norm, True)
# do a groupby to see how our clusters perform
km.groupby('cluster').count().sort_values('x', ascending = False)

"""Let us observe the distribution of data from these 8 clusters by barplots first."""

clusters_dist = km.groupby('cluster').count().sort_values('x', ascending = False)
clusters_dist.reset_index(drop = False, inplace = True)
clusters_dist['percentage'] = (clusters_dist.x/clusters_dist.x.sum())*100
sns.catplot(x= 'cluster', y = 'percentage', hue = 'cluster', data = clusters_dist, kind = 'bar', height = 6, aspect = 1.5, dodge = False )

"""If you are wondering why the cluster 3 has too little data cases, we can check its principal components value by printing it out, which we will observe that cluster 3 has an extreme high z value. To analyze what the z value is, we can print out he PCA components to see what features in our seven indicators are culpable for resulting such a large z value."""

km[km['cluster'] ==3]

pca_component_analysis

"""As we can see from the last segment of code, in the third row of the array (which is a explained analysis of what constitutes "z" value), the last second entry (column = 5) contributes the most, 0.99528263, which is actually **salt**. 

By further printing out the seven indicators of all the data cases in assigned to cluster 3, we can see that these values, undoubtly has a **too large value of salt** that they comprises of a unique cluster for them. 

In the next step, the dataframe printed below joins the principal components (x, y, z) of the data cases we used to do k-means with its original seven indicators   ( 'saturated-fat_100g', 'fat_100g', 'carbohydrates_100g', 'sugars_100g', 'proteins_100g', 'salt_100g', 'energy_100g').
"""

pca_original_combined= km[km['cluster'] ==3].reset_index(drop = False).merge(pd.DataFrame(scaler_returned.inverse_transform(X_norm)).reset_index(drop = False), on = 'index')
pca_original_combined

"""This is a quick check (only involve some products which have unique product name) to reassure that the column 5 is exactly the column for salt_100g. And these product do have a high-salt property such that they are being clustered together by k-means."""

food_df_clustering[food_df_clustering['product_name'].apply(lambda x: x in {'Crispy Fried Onions, White Cheddar','String Cheese Snacks','Bastoncini Whole Wheat','Small Potato Dumplings, Potato Gnocchetti','Chiles Habaneros Rojos','Sausage Patties, Veggie Sausage Patties',})] \
[['product_name','saturated-fat_100g','fat_100g','carbohydrates_100g','sugars_100g','proteins_100g','salt_100g','energy_100g']]

"""### WordCloud for selected clusters"""

import nltk
nltk.__version__

from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords = set(stopwords.words('english'))

nltk.corpus.stopwords

# TODO: tokenize and flatten
def tokenize_content(content):
  tokens = nltk.word_tokenize(content)
  alpha_words = [word.lower() for word in tokens if word.isalpha()]
  removed_alpha_words = [word for word in alpha_words if word not in stopwords]

  return  removed_alpha_words

from wordcloud import WordCloud

#create a wordcloud for different clusters
def create_wordcloud(df_series, clusterNum):
  # TODO: tokenize and flatten
  df_series = df_series.astype(str)
  name = df_series.tolist()
  top_tokens_list = [] 


  for item in name:
    top_tokens_list.append(tokenize_content(item))

  top_tokens = [item for sublist in top_tokens_list for item in sublist]

  # make a word cloud for top tokens 

  plt.subplots(figsize = (10,5))

  wordcloud = WordCloud (
                    background_color = 'white',
                    width = 300,
                    height = 200
                        ).generate(' '.join(top_tokens))
  plt.imshow(wordcloud) # image show
  plt.axis('off') # to off the axis of x and y
  plt.title('wordcloud for cluster ' + clusterNum) # to off the axis of x and y
  plt.show()

#for all the clusters that in km(not outliers)
for clusterNum in clusters_dist.cluster:
  
  cluster_df = km[km['cluster'] == clusterNum]

  create_wordcloud(cluster_df['name'], str(clusterNum))

"""A little guess:

Cluster 0: candy/strawberry/honey **sweets**

Cluster 1: beef jerky/protein bar **protein products?**

Cluster 2: dark chocolate/potato chip **snacks?**

Cluster 3: **salty!!**

Cluster 4: olive oil/ oil **oily**

Cluster 5: sauce/black beans/organic **organic**

Cluster 6: cheese/peanut butter/butter **high energy**

Cluster 7: ice cream/macaroni/bread **high carbo products?**

## Hierchical Clustering

In the second section of our clustering, we apply agglomerative clustering which is a type of hierarchical clustering. Typically, hierarchical clustering gives us  insights on data relations directly through dendregrams. However, one of its disadvantages is that it is not feasible for us to determine the cluster numbers like k-means.

For the sake of visual clarity and running time, we will only choose 100 data cases when creating features to pass into our models. Like the previous clustering method, we choose seven renowned food indicators as our features.
"""

from sklearn.preprocessing import normalize

def create_features(df):
    df = df[[
        'saturated-fat_100g','fat_100g','carbohydrates_100g','sugars_100g','proteins_100g','salt_100g','energy_100g' # or something else
        ]].sample(1000, random_state=66)
  
    df.dropna(how = 'all', axis = 0, inplace=True)
    df.fillna(0, inplace = True)
    x = df.values # numpy array
    scaler = preprocessing.StandardScaler()

    #scale the data
    x_scaled = scaler.fit_transform(x)
    #normalize the data
    df_normalized = normalize(x_scaled)

    # Reduce the dimensionality of data to 3 features

    pca = PCA(n_components=3)
    df_pca = pca.fit_transform(df_normalized)
    df_pca = pd.DataFrame(df_pca)
    df_pca.columns = ['P1', 'P2', 'P3']



    return pd.DataFrame(df_pca)

X_hier_clustering = create_features(food_df_clean)

"""Aftering prepocessing our data, it is time to pass them into our model of hierchical clustering with help of scipy."""

import scipy.cluster.hierarchy as shc

plt.figure(figsize=(12, 7))  
plt.title("Dendrograms")  
dend = shc.dendrogram(shc.linkage(X_hier_clustering, method='ward'))

"""### Choosing cluster number

From this graph, the x-axis are the samples and the y-axis are the distance between the samples. The vertical line that has maximum distance is the blue line and hence we can decide a threshold within the blue line and cut the dendrogram:
"""

plt.figure(figsize=(12, 7))  
plt.title("Dendrograms")  
dend = shc.dendrogram(shc.linkage(X_hier_clustering, method='ward'))
plt.axhline(y=17, color='r', linestyle='--')
plt.axhline(y=24.5, color='r', linestyle='--')

"""The optimal number of clusters is equal to the number of vertical lines going through the horizontal line. We can observe that the blue line only has two vertical lines across itself. So the maximum cluster to choose within this 100 data cases will be **2**."""

from sklearn.cluster import AgglomerativeClustering
cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')  
cluster.fit_predict(X_hier_clustering)
labels = cluster.labels_

labels

X_hier_clustering

plt.figure(figsize =(8, 8))
plt.scatter(X_hier_clustering['P1'], X_hier_clustering['P2'], c = labels, cmap ='rainbow')
plt.title("Agglomerative Hierarchical Clusters - Scatter Plot", fontsize=18)
plt.show()

"""## Gaussian Mixture Models

Gaussian Mixture Model is very similar to the application of k-means. However, it is more advanced than k-means. In k-means, we usually choose the centroids only to and calculate the distance between the centroids and data to assign clusters. 

However, it doesn't take into account of circumstances when multiple centroids might have same centroids or the centroid is not enough to represent the clusters.

Therefore, in Gaussian Mixture Models, we have more parameters when we are building clusters: the mean and the covariance, to describe the position and shape of each cluster. First, we start with assigning random parameters to each clusters ( (ie the mean, covariance and weighting). When assigning data to clusters, we calculate the likehood of each data belonging to a cluster according to current data. Then, we recalculate the cluster's parameters until it reaches a stable condition.

First, we start by creating features for this model.
"""

# seven food indicators for modelling
filtered_columns = ['saturated-fat_100g','fat_100g','carbohydrates_100g','sugars_100g','proteins_100g','salt_100g','energy_100g']

# Cleaning if any values are empty in a row
food_df_clean["isempty"] = np.where(food_df_clean[filtered_columns].isnull().sum(axis=1) >= 1, 1, 0)
# Estimation of cleaning proportion
percentage = (food_df_clean.isempty.value_counts()[1] / food_df_clean.shape[0]) * 100
print("Percentage of incomplete tables: " + str(percentage))
print(food_df_clean.isempty.value_counts())
# Cleaning
df_cleaned = food_df_clean[food_df_clean.isempty==0]
# Check if cleaning is successful
df_cleaned.isnull().sum()

"""Let's apply the model. There are many ways that we can choose cluster number for GMM, such as BOC/AOC. For the sake of simplicity and running time, we will choose the cluster number to be the same as k-means cluster number **6**."""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.mixture import GaussianMixture
# 
# #creatig features
# features = ['saturated-fat_100g','fat_100g','carbohydrates_100g','sugars_100g','proteins_100g','salt_100g','energy_100g']
# X_train = df_cleaned[features].values
# 
# #choosing cluster number
# n_clusters = 8
# 
# #create the model
# max_iter = 100
# model = GaussianMixture(n_components=n_clusters, covariance_type="full", n_init = 5, max_iter = max_iter)
# #fit model on data
# model.fit(X_train)
# 
# #predict cluster number for the date
# results = df_cleaned
# results["cluster"] = model.predict(X_train)

"""After running the model, let us visualize these clusters by creating wordcould for them."""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import math
# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
# def make_word_cloud(data, cluster, subplotax, title):
#     # Get words from the product name 
#     words = data[data.cluster==cluster]["product_name"].apply(lambda l: l.lower().split() if type(l) == str else '')
#     # Add to a pandas series
#     cluster_words=words.apply(pd.Series).stack().reset_index(drop=True)
#     # Split and join
#     text = " ".join(w for w in cluster_words)
# 
#     # Create and generate a word cloud image:
#     wordcloud = WordCloud(max_font_size=30, max_words=30, background_color="white", colormap="YlGnBu").generate(text)
# 
#     # Display the generated image:    
#     subplotax.imshow(wordcloud, interpolation='bilinear')
#     subplotax.axis("off")
#     subplotax.set_title(title,fontweight="bold", size=20)
#     return subplotax
#

rows = math.ceil(n_clusters/2)
fig, ax = plt.subplots(rows,2, figsize=(20,10))
for m in range(rows):
    for n in range(2):
        cluster = m*2+ n
        title = "Cluster " + str(cluster) 
        make_word_cloud(results, cluster, ax[m,n], title)

for cluster in range(0,n_clusters):
    display("Cluster "+str(cluster))
    display(results[results['cluster'] == cluster][features].describe())

"""Most of the clusters are quite clear and similar to what we have in k-means, here's a little fun guess of me ✨✨:

Cluster 0: dark chocolate/potato chip **snacks?**
(high enerygy, high carbo, high sugar, high fat)

Cluster 1: nonfat yogurt/mango/orange **healthy food?**
(low everything)

Cluster 2: macaroni/spagetti/rice/grain **carbo!**
(high carbo, high energy)

Cluster 3: strawberry/mango/nonfat yougurt **super healthy food and fruits**
(low everything, even lower than cluster 1)

Cluster 4: cheese/olive oil/butter **For cuisine?**
(high enerygy, high carbo, high sugar, high fat, high protein)

chicken/turkey breast/tomato sauce **meat&soup?**
(low enerygy, low carbo, low sugar, low fat, median protein)

Cluster 5: Prickly and syrup! which is exactly what we dropped in k-means!

Cluster 6: ice cream/cake/biscuit **bakings?**
(high enerygy, high carbo, low sugar, low fat)

Cluster 7: beef jerky/protein bar **protein products?**
(medium energy, high carbo, high sugar, high protein, medium fat)

#**Supervised Machine Learning**

In this section, we used supervised machine learning methods to map functions from features to selected targets. We included multiple methods for predictions of both categorical and continuous values, for specific topic that is meaningful and worths analysis. Within each part, we tried different methods, examined their performance and further improved them. For each part, we used different preprocessing methods to deal with different problems, including multicollinearity, missing data, and imbalance in the training set. During this process, we also did some comparisons to better understand the models and methods we were using.

We first conducted binary classifications for plant-based foods and sugary foods, as we noticed their importance among the whole dataset in our previous sections. In these two parts, we applied common methods for classification, including Logistic Regression, Decision Trees, Random Forests, and Neural Network.

Then we paid attention to an essential content of food products, energy. For this feature, we decided to do a multi-classification with Decition Trees and Random Forests.

For the last part, we tried to predict the Nutrition Score, which is a continuous numeric value, of a food product based on its contents. We used Linear Regression and tried different Regularizations for possible improvements.

##Plant-based Foods: Binary Classification

Theoretically, it is possible and conceivable to discern if a food product is plant-based or not according to its components. In this section, we attempted to classify products into two categories, plant-based and not plant-based, according to the products' quantitative contents.

###Data Preprocessing

####Cleaning

We dropped those columns whose values are not numeric and those irrelavant to this part of study.
"""

food_contents_df = food_df.drop(columns=['code','url','creator','created_t','created_datetime','last_modified_t','last_modified_datetime'
                                                      ,'generic_name','quantity','packaging','packaging_tags','brands','brands_tags','categories'
                                                      ,'categories_tags','origins','origins_tags','manufacturing_places','manufacturing_places_tags'
                                                      ,'labels','labels_tags','labels_en','emb_codes','emb_codes_tags','first_packaging_code_geo'
                                                      ,'cities','cities_tags','purchase_places','stores','countries','countries_tags','countries_en'
                                                      ,'ingredients_text','allergens','allergens_en','traces','traces_tags','traces_en'
                                                      ,'serving_size','additives_n','additives','additives_tags','additives_en'
                                                      ,'ingredients_from_palm_oil_n','ingredients_from_palm_oil_tags'
                                                      ,'ingredients_that_may_be_from_palm_oil_n'
                                                      ,'ingredients_that_may_be_from_palm_oil_tags','nutrition_grade_fr','pnns_groups_1','pnns_groups_2'
                                                      ,'states','states_tags','states_en','main_category','main_category_en','image_url','image_small_url'
                                                      ,'ph_100g','carbon-footprint_100g','nutrition-score-fr_100g'])

#drop those rows without category
plant_food_df = food_contents_df.dropna(subset=['categories_en'])

plant_food_df['categories_en'].unique()

#fill null-value cells with 0
plant_food_df = plant_food_df.fillna(0)

plant_food_df = plant_food_df.dropna(subset=['categories_en'])

"""#### Feature Engineering

We create a 'plant-based' column to indicate whether a product is plant-based or not. This information can be found in the category column.
"""

#define a function applied on 'categories_en' column to determine whether a product is plant-based
def ifPlantBased(x):
  if "Plant-based" in x:
    return 1.0
  else:
    return 0.0

plant_food_df['plant_based'] = plant_food_df['categories_en'].apply(ifPlantBased)

"""####Set Up for Modeling"""

#store the columns to be used as features in a DataFrame called "features" 
features = plant_food_df.drop(columns=['plant_based','product_name', 'categories_en', 'energy_100g', 'energy-from-fat_100g','nutrition-score-uk_100g'])
#store the binary classification target variable as "labels"
labels = plant_food_df['plant_based']

from sklearn.model_selection import train_test_split
#conduct 80/20 train-test split
seed = 42
x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = seed)

"""###Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Initialize model with default parameters and fit it on the training set
reg = LogisticRegression()
reg.fit(x_train, y_train)
# Use the model to predict on the test set and save these predictions as `y_pred`
y_pred = reg.predict(x_test)
# accuracy
acc = accuracy_score(y_test, y_pred)

acc

"""####  PCA

To reduce dimensionality and deal with multicollinearity, we applied Principal Component Analysis before regression, attempting to improve the proformance of our model.

**Initial PCA**
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# PCA is not scale-invariant - address scale-invariance
scaler = StandardScaler()
x_train_sc = scaler.fit_transform(x_train)
x_test_sc = scaler.transform(x_test)
# Instantiate and Fit PCA
pca = PCA() #all components are kept
pca.fit(x_train_sc)

#explained variance ratios
explained_variance_ratios = pca.explained_variance_ratio_
#cumulative explained variance ratios
cum_evr = [explained_variance_ratios[0]]
for i in range(1, len(explained_variance_ratios)):
  cum_evr.append(explained_variance_ratios[i] + cum_evr[i-1])

# plot to find optimal number of components to use 
plt.figure().set_figwidth(20)
plt.plot(np.arange(1, len(cum_evr) + 1), cum_evr)
plt.xticks(range(1, len(cum_evr) + 1))
plt.axhline(y=0.8, color='r')
plt.axvline(x=42, color='0.3', linestyle='--')  #mark the cross point
plt.title('Cumulative Explained Variance Ratio Against Number of Components')
plt.xlabel('Number of components')
plt.xlabel('Cumulative explained variance ratio')
plt.show()

"""**Final PCA**

We maintained only principal components we selected.
"""

# Refit and transform on training with parameter n (as deduced from the last step) 
pca = PCA(n_components=42)
x_train_pca = pca.fit_transform(x_train_sc)

# Transform on Testing Set and store it as `x_test_pca`
x_test_pca = pca.transform(x_test_sc)

"""**Modeling and Training**

Conduct Logistic Regression on re-scaled training and test datasets after PCA.
"""

# Initialize `log_reg_pca` model with default parameters and fit it on the PCA transformed training set
log_reg_pca = LogisticRegression()
log_reg_pca.fit(x_train_pca, y_train)

# Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`
y_pred = log_reg_pca.predict(x_test_pca)

# Find the accuracy and store the value in `test_accuracy`
test_accuracy = accuracy_score(y_test, y_pred)

test_accuracy

"""The accuracy of this model is passable, but not very satisfying, so we turned to other methods for a possibly better performance. Also, Logistic Regression is somehow abstract and not very intuitive to understand, while another method does better in this domain.

###Decision Tree

Decision Tree is another popular algorithm for classification. It learns by representing data and decision process as a tree, where each split point (non-leaf node of the tree) denotes a feature and each leaf node denotes a class label. One of its characteristics is scale-invariant, meaning that it does not need normalization and scaling beforehand. Moreover, the Decition Tree model is intuitive, easy to visualize and relatively easy to understand.

SciKit Learn has a well developed package for Decision Tree model, which is easy to apply.
"""

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
dt = dt.fit(x_train, y_train)

y_pred = dt.predict(x_test)

"""Due to the huge body of the current tree model, not only plotting this decision tree is time-comsuming, but the plot itself tends to have extremely low readability. Therefore, we will leave this part for a slimmer tree model."""

# accuracy
train_acc = dt.score(x_train,y_train)
test_acc = dt.score(x_test,y_test)

train_acc

test_acc

"""Taking a look at the train_acc and test_acc, there is a reasonable concern about **overfitting** since the accuracy of prediction on the training dataset is obviously higher than the accuracy of prediction on the testing dataset.

It is not surprising for a Decision Tree model. To address this, we are going to try PCA, limiting depth and ensembling (Random Forest) to help improve the performance of the model.

####Limit Depth

**Find an optimal depth**

To find a optimized value for the parameter max_depth, loop through reasonable values and calculate the accuracies for each model.
"""

lis_depth = []
lis_train_acc = []
lis_test_acc = []

for i in range(1, 31):
  # develop and train model
  op_dt = DecisionTreeClassifier(criterion="entropy", max_depth=i)
  op_dt = op_dt.fit(x_train, y_train)
  # accuracy
  train_acc = op_dt.score(x_train,y_train)
  test_acc = op_dt.score(x_test,y_test)
  # save values to corresponding lists
  lis_depth.append(i)
  lis_train_acc.append(train_acc)
  lis_test_acc.append(test_acc)

# visualize
plt.plot(lis_depth, lis_train_acc, label='train_acc')
plt.plot(lis_depth, lis_test_acc, label='test_acc')
plt.title('Decision Tree: Accuracy vs Depth')
plt.xlabel('max_depth')
plt.ylabel('accuracy')
plt.legend()
plt.show()

"""With the value of depth increasing, the model obviously becomes more and more overfitting. 

After max_depth reaching 10, the difference between train_acc and test_acc increases significantly, which indicates that the model becomes increasingly overfitting, while the model rising depth only provides limited improvement to the accuracy of prediction on testing dataset. Based on this, we decided to set max_depth as 10.

**Establish Model with Optimized Depth**
"""

# develop and train model
op_dt = DecisionTreeClassifier(criterion="entropy", max_depth=10)
op_dt = op_dt.fit(x_train, y_train)
# accuracy
train_acc = op_dt.score(x_train,y_train)
test_acc = op_dt.score(x_test,y_test)

train_acc

test_acc

"""By reducing the depth of the decision tree, we slightly compromised our testing accuracy, but nicely solved the problem of overfitting."""

pip install graphviz

"""**Visualization**"""

import graphviz 
from sklearn.tree import export_graphviz

feature_names=list(features.columns)
class_names = ['Plant-based', 'Not Plant-based']

dot_data = export_graphviz(op_dt, out_file=None, 
                     feature_names=feature_names,  
                     class_names=class_names,  
                     filled=True, rounded=True,  
                     special_characters=True)  
graph = graphviz.Source(dot_data)  
graph

"""###Random Forest

####Limit Depth

**Find an optimal depth**

We also want to determine an optimal value for max_depth parameter while establishing the random forest model. Again, we are using a loop to record accuracies for models with different depths.
"""

from sklearn.ensemble import RandomForestClassifier

lis_depth = []
lis_train_acc = []
lis_test_acc = []

for i in range(1, 31):
  # develop and train model
  rf = RandomForestClassifier(n_estimators = 20, max_depth=i)
  rf = rf.fit(x_train, y_train)
  # accuracy
  train_acc = rf.score(x_train,y_train)
  test_acc = rf.score(x_test,y_test)
  # save values to corresponding lists
  lis_depth.append(i)
  lis_train_acc.append(train_acc)
  lis_test_acc.append(test_acc)

# visualize
plt.plot(lis_depth, lis_train_acc, label='train_acc')
plt.plot(lis_depth, lis_test_acc, label='test_acc')
plt.title('Random Forest: Accuracy vs Depth')
plt.xlabel('max_depth')
plt.ylabel('accuracy')
plt.legend()
plt.show()

"""With the value of depth increasing, the model still becomes more and more overfitting. However, we see significant improvement in the accuracy of predicion on test dataset. 

According to the plot, we decided to set max_depth as 14, where the difference between train_acc and test_acc is small and prediction on test dataset has a relatively good performance.

**Establish Model with Optimized Depth**
"""

# develop and train model
rf = RandomForestClassifier(n_estimators = 20, max_depth=14) 
rf = rf.fit(x_train, y_train)
# accuracy
train_acc = rf.score(x_train,y_train)
test_acc = rf.score(x_test,y_test)

train_acc

test_acc

"""##Sugary Foods: Binary Classification

We used the decision tree first because it has speed and can be explained. Then we tried random forest and neural networks to improve accuracy.

When choosing features, we deleted the columns of 'sugars_100g' and 'carbohydrates_100g' columns. Carbohydrates are sugars are directly related to sugar. Sugar is a type of carbohydrate found naturally in many of the foods we eat.

###Data Preprocessing

####Cleaning
"""

food_contents_df = food_df.drop(columns=['code','url','creator','created_t','created_datetime','last_modified_t','last_modified_datetime'
                                                      ,'generic_name','quantity','packaging','packaging_tags','brands','brands_tags','categories'
                                                      ,'categories_tags','origins','origins_tags','manufacturing_places','manufacturing_places_tags'
                                                      ,'labels','labels_tags','labels_en','emb_codes','emb_codes_tags','first_packaging_code_geo'
                                                      ,'cities','cities_tags','purchase_places','stores','countries','countries_tags','countries_en'
                                                      ,'ingredients_text','allergens','allergens_en','traces','traces_tags','traces_en'
                                                      ,'serving_size','additives_n','additives','additives_tags','additives_en'
                                                      ,'ingredients_from_palm_oil_n','ingredients_from_palm_oil_tags'
                                                      ,'ingredients_that_may_be_from_palm_oil_n'
                                                      ,'ingredients_that_may_be_from_palm_oil_tags','nutrition_grade_fr','pnns_groups_1','pnns_groups_2'
                                                      ,'states','states_tags','states_en','main_category','main_category_en','image_url','image_small_url'
                                                      ,'ph_100g','carbon-footprint_100g','nutrition-score-fr_100g'])

#drop those rows without category
sugar_food_df = food_contents_df.dropna(subset=['categories_en'])
sugar_food_df['categories_en'].unique()

sugar_food_df = sugar_food_df.dropna(subset=['categories_en'])
# fill nan with 0 in each row
sugar_food_df=sugar_food_df.fillna(0)

"""#### Feature Engineering

We create sugar column. This below checks if a product is sugarly or not. This information can be found in the category column
"""

# Check if it is sugar based
def ifSugarBased(x):
  if "Sugary" in x:
    return 1.0
  else:
    return 0.0

sugar_food_df['sugar'] = sugar_food_df['categories_en'].apply(ifSugarBased)

"""####Identify Imbalance

As shown below, we can see that the data is imbalanced for the label. We have much more data that fall into non sugary category.
"""

# we can see that we have much more non-sugar product
sugar_food_df['sugar'].value_counts()

from matplotlib import pyplot as plt
sugar_food_df['sugar'].value_counts().plot(kind="bar", title="test")

plt.title("Whether or not a product is sugary")
plt.xlabel("Sugar")
plt.ylabel("The number of count")

"""### Decision Tree

Given the dataset, we want to build a decision tree classifier in order to classify whether a product is sugary or not based on certain features. 

The decision tree model we use greedily determines the attribute on which to split by maximizing information gain.Then, we will estimate how accurately the classifier or model can predict the type of sugarly product. We got a classification rate of 92% before the oversampling, considered as good accuracy.

#### Before oversampling

We first just implement the model as normal and do not consider data imbalance for our label.
"""

# delete them because they are object
sugar_food_df.dtypes[sugar_food_df.dtypes != 'int64'][sugar_food_df.dtypes != 'float64']

#store the columns to be used as features in a DataFrame called "features" 
features = sugar_food_df.drop(columns=['sugar','product_name', 'categories_en', 'sugars_100g', 'carbohydrates_100g', 'energy_100g', 'energy-from-fat_100g','nutrition-score-uk_100g'])
#store the target variable as "labels"
labels = sugar_food_df['sugar']

from sklearn.model_selection import train_test_split
#conduct 70/30 train-test split
seed = 40
x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state = seed)

# Load libraries
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics

# Create Decision Tree classifer object
clf = DecisionTreeClassifier()

# Train Decision Tree Classifer
clf = clf.fit(x_train,y_train)

#Predict the response for test dataset
y_pred = clf.predict(x_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

"""####Balancing Training Data
We use oversampling to solve the problem of data imbalance. We have a large number of trainning examples that fall into non-sugar class. So we add some of underrepresented examples to solve the problem and run the decision tree.
"""

# use down-sampling to solve data imbalance 
from imblearn.over_sampling import RandomOverSampler
rus = RandomOverSampler(random_state=42)
x_res, y_res = rus.fit_resample(x_train, y_train)

# Create Decision Tree classifer object
clf = DecisionTreeClassifier()

# Train Decision Tree Classifer
clf = clf.fit(x_res,y_res)

#Predict the response for test dataset
y_pred = clf.predict(x_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

"""### Random Forest

We use a bagging ensemble method, random forest to increase the accuracy of the model. But at the same time, it will take longer time.

We also use `feature_importances_`to reflect the order of the columns in my features dataframe so we can see which feature is more important. It is interesting to see that saturated fat (saturated-fat) has one of the highest predicting power (18%) in whether or not a type of product is sugar.

When we instantiate model with 100 decision trees, this model has an accuracy score of 94% on the test data before the oversampling, which  seems impressive and better than the decision tree model.

#### Before oversampling

As before, we first just implement the model as normal and do not consider data imbalance for our label.
"""

# Import the model we are using
from sklearn.ensemble import RandomForestClassifier
# Instantiate model with 10 decision trees
rf = RandomForestClassifier(n_estimators = 100, random_state = 42)
# Train the model on training data
rf.fit(x_train, y_train);

# Use the forest's predict method on the test data
predictions = rf.predict(x_test)

# View accuracy score
print("Accuracy:",metrics.accuracy_score(y_test, predictions))

"""####Balancing Training Data
We use oversampling to solve the problem of data imbalance. We have a large number of trainning examples that fall into non-sugar class. So we add some of underrepresented examples to solve the problem and run the random forest.
"""

from imblearn.over_sampling import RandomOverSampler
rus = RandomOverSampler(random_state=42)
x_res, y_res = rus.fit_resample(x_train, y_train)

rf = RandomForestClassifier(n_estimators = 100, random_state = 42)
# Train the model on training data
rf.fit(x_res, y_res);

predictions = rf.predict(x_test)

print("Accuracy:",metrics.accuracy_score(y_test, predictions))

"""####Feature Importances

We do feature importance and try to see which feature has more power in determining whether or not a product is sugary.
"""

# check which feature is more important
importances = rf.feature_importances_

i = 0
while i < len(x_train.columns):
  print(f"The importance of feature '{x_train.columns[i]}' is {round(importances[i]*100, 2)}%.")
  i = i + 1

"""### FeedForward Neural Network

We use the first and simplest type of artificial neural network devised to increase the accuracy of the model. But at the same time, it will take longer time.

This model has an accuracy score of 91% on the test data before the oversampling, which is good. However, the result is not as good as the results we derived from the random forest model. Therefore, we got the best result by using random forest model in the last session.

#### Normalize Data

We normalize data before implementing the model. The model is affected by the value range of features. We did not do this for decision trees and random forests because they are scale-invariant.
"""

from keras import models, layers, optimizers, regularizers
from sklearn import model_selection, preprocessing
import tensorflow as tf

x_train_normalize = preprocessing.normalize(x_train)
x_test_normalize = preprocessing.normalize(x_test)

"""#### Before oversampling

As before, we first just implement the model as normal and do not consider data imbalance for our label. Since the result is not as good as the result of random forest, we will not go furhter and do oversampling for this model.
"""

hidden_units = 100     # how many neurons in the hidden layer
activation = 'relu'   # activation function for hidden layer
l2 = 0.01             # regularization - how much we penalize large parameter values
learning_rate = 0.01  # how big our steps are in gradient descent
epochs = 5          # how many epochs to train for
batch_size = 64     # how many samples to use for each gradient descent update

# create a sequential model
model = models.Sequential()

# add the hidden layer
model.add(layers.Dense(input_dim=x_train.shape[1],
                       units=hidden_units, 
                       activation=activation))

# add the output layer
model.add(layers.Dense(input_dim=hidden_units,
                       units=1,
                       activation='sigmoid'))

# define our loss function and optimizer
model.compile(loss='binary_crossentropy',
              # Adam is a kind of gradient descent
              optimizer=optimizers.Adam(lr=learning_rate),
              metrics=['accuracy'])

# train the parameters
history = model.fit(x_train_normalize, y_train, epochs=5, batch_size=batch_size)

# evaluate accuracy
train_acc = model.evaluate(x_train_normalize, y_train, batch_size=32)[1]
test_acc = model.evaluate(x_test_normalize, y_test, batch_size=32)[1]
# print('Training accuracy: %s' % train_acc)
print('Testing accuracy: %s' % test_acc)

"""##Energy of Foods: Multi-Classification

Energy is one of the most essential factor in assessing the nutritional value of foods, and thus receives fair amount of attention. Nowadays, people are becoming increasingly concerned about the energy content of food products, not only because they want to maintain their physical performance, but also because they want to maintain healthy weights and body shapes. Controlling the quantity of energy consumed is the most important component of losing weight, which is a long-lasting hot topic on social media platforms. Therefore, when consumers are deciding whether or not to purchase a food product, they undoubtedly take its energy content into consideration. Furthermore, as weight loss applications grow more popular, many people utilize them to help calculate the amount of energy they consume on a daily basis, which makes energy prediction based on food contents more demanded.

We believe that it is possible and conceivable to determine the energy level a food product according to its components. Thus in this section, we tried to classify products into three categories, namely high-energy, medium-energy, and low-energy, according to their quantitative contents.

###Data Preprocessing

#### Cleaning

We dropped those columns whose values are not numeric and those irrelavant to this part of study.
"""

food_energy_df = plant_food_df.drop(columns=['product_name','categories_en', 'plant_based', 'energy-from-fat_100g','nutrition-score-uk_100g'])

food_energy_df.columns

"""#### Feature Engineering

We split energy values uniformly into 3 level groups so as to make sure that each group has same or similar number of instances and avoid imbalance. Based on this idea, we calculate quantiles as the thresholds used to assign levels.
"""

lo = food_energy_df['energy_100g'].quantile(q=1/3)
hi = food_energy_df['energy_100g'].quantile(q=2/3)

"""As calculated above, 2/3 products are with energy lower than the *hi* threshold, and 1/3 products are with energy lower than the *lo* threshold. According to this statistical results, we split energy into three levels: 

> high (>hi),

> medium (in the range between lo and hi, inclusively), and 

> low (<lo). 

We then created a new column called 'energy_level' to indicate whether a product is high-energy (type: 1), medium-energy (type: 2) or low-energy (type: 3).
"""

def energyLevel(x):
  if x > hi:
    return 1
  elif x < lo:
    return 3
  else:
    return 2

food_energy_df['energy_level'] = food_energy_df['energy_100g'].apply(energyLevel)

food_energy_df = food_energy_df.drop(columns=['energy_100g'])

"""####Set up for Modeling"""

#store the columns to be used as features in a DataFrame called "features" 
features = food_energy_df.drop(columns=['energy_level'])
#store the binary classification target variable as "labels"
labels = food_energy_df['energy_level']

#conduct 80/20 train-test split
seed = 42
x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = seed)

"""###Decision Tree"""

# modeling and training
dt = DecisionTreeClassifier()
dt = dt.fit(x_train, y_train)
# accuracy
train_acc = dt.score(x_train,y_train)
test_acc = dt.score(x_test,y_test)

print(f'Training Accuracy: {train_acc}.')
print(f'Test Accuracy: {test_acc}.')

"""A slight sign of overfitting can be seen from the difference between the training accuracy and the test accuracy, so we tried to address this in the below section.

####Limit Depth

To address overfitting, we need to reduce the complexity of our model. One of the most efficient way of doing this on a Decition Tree model is to limit the maximum depth of the tree.
"""

lis_depth = []
lis_train_acc = []
lis_test_acc = []

for i in range(1, 31):
  # develop and train model
  op_dt = DecisionTreeClassifier(criterion="entropy", max_depth=i)
  op_dt = op_dt.fit(x_train, y_train)
  # accuracy
  train_acc = op_dt.score(x_train,y_train)
  test_acc = op_dt.score(x_test,y_test)
  # save values to corresponding lists
  lis_depth.append(i)
  lis_train_acc.append(train_acc)
  lis_test_acc.append(test_acc)

# visualize
plt.plot(lis_depth, lis_train_acc, label='train_acc')
plt.plot(lis_depth, lis_test_acc, label='test_acc')
plt.title('Decision Tree: Accuracy vs Depth')
plt.xlabel('max_depth')
plt.ylabel('accuracy')
plt.legend()
plt.show()

"""We chose 10 as the largest max_depth with tolerable difference between train_acc and test_acc, which indicates the degree of overfitting."""

# develop and train model
op_dt = DecisionTreeClassifier(criterion="entropy", max_depth=10)
op_dt = op_dt.fit(x_train, y_train)
# accuracy
train_acc = op_dt.score(x_train,y_train)
test_acc = op_dt.score(x_test,y_test)

print(f'Training Accuracy: {train_acc}.')
print(f'Test Accuracy: {test_acc}.')

"""We successfully decreased the difference between training accuracy and test accuracy from 0.04 to 0.01.

###Random Forest

We wanted to see if emsembling makes our model better for this particular classification. Random Forest is a way of bagging. It creates many trees on subsets of the data and combines the output by majority votes. In reduces overfitting in decision trees and improves the accuracy. Also, it is robust to outliers, though we did not noticed this problem in this classification.
"""

# develop and train model
rf = RandomForestClassifier(n_estimators = 20) 
rf = rf.fit(x_train, y_train)
# accuracy
train_acc = rf.score(x_train,y_train)
test_acc = rf.score(x_test,y_test)

print(f'Training Accuracy: {train_acc}.')
print(f'Test Accuracy: {test_acc}.')

"""Compared with the Decision Tree model, this Random Forest model suffers less from overfitting when using default parameters, without additionally limiting maximum depth.

####Limit Depth
"""

lis_depth = []
lis_train_acc = []
lis_test_acc = []

for i in range(1, 31):
  # develop and train model
  op_rf = RandomForestClassifier(n_estimators = 20, max_depth=i)
  op_rf = op_rf.fit(x_train, y_train)
  # accuracy
  train_acc = op_rf.score(x_train,y_train)
  test_acc = op_rf.score(x_test,y_test)
  # save values to corresponding lists
  lis_depth.append(i)
  lis_train_acc.append(train_acc)
  lis_test_acc.append(test_acc)

# visualize
plt.plot(lis_depth, lis_train_acc, label='train_acc')
plt.plot(lis_depth, lis_test_acc, label='test_acc')
plt.title('Random Forest: Accuracy vs Depth')
plt.xlabel('max_depth')
plt.ylabel('accuracy')
plt.legend()
plt.show()

# develop and train model
op_rf = RandomForestClassifier(n_estimators = 20, max_depth=13) 
op_rf = op_rf.fit(x_train, y_train)
# accuracy
train_acc = op_rf.score(x_train,y_train)
test_acc = op_rf.score(x_test,y_test)

print(f'Training Accuracy: {train_acc}.')
print(f'Test Accuracy: {test_acc}.')

"""By limiting the max_depth of the Random Forest model, we slightly compromised our testing accuracy, but reduced the degree of overfitting.

In this particular classification, Random Forest showed slight improvement on dealing with overfitting compare with Decision Tree, but showed no significant advantages considering overall performance.

##Nutrition Score Prediction: Regression

###Data Preprocessing
"""

#drop unused columns
score_food_df = food_contents_df.drop(columns=['product_name', 'categories_en'])
#drop those rows without category
score_food_df = score_food_df.dropna(subset=['nutrition-score-uk_100g'])
#fill nan
score_food_df = score_food_df.fillna(0)

# to display full list (disable truncate view)
pd.set_option("display.max_rows", None)

for column in score_food_df.columns:
  print(column)

score_food_df.dtypes

#store the columns to be used as features in a DataFrame called "features" 
features = score_food_df.drop(columns=['nutrition-score-uk_100g'])
#store the binary classification target variable as "labels"
labels = score_food_df['nutrition-score-uk_100g']

from sklearn.model_selection import train_test_split
#conduct 80/20 train-test split
seed = 42
x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = seed)

"""###Linear Regression"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, accuracy_score, mean_squared_error

# Initialize model with default parameters and fit it on the training set
reg = LinearRegression()
reg.fit(x_train, y_train)
# Use the model to predict on the test set and save these predictions as `y_pred`
y_pred = reg.predict(x_test)
# Find the R-squared score and RMSE score
linear_score = r2_score(y_test, y_pred)
linear_score_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
#score(accuracy)
train_acc =reg.score(x_train,y_train)
test_acc =reg.score(x_test,y_test)

print(f'R2 score: {linear_score}.')
print(f'RMSE score: {linear_score_rmse}.')
print(f'Training Accuracy: {train_acc}.')
print(f'Test Accuracy: {test_acc}.')

"""###Lasso Regularized Linear Regression"""

from sklearn.linear_model import Lasso

# Initialize model with default parameters and fit it on the PCA transformed training set
lasso = Lasso(alpha = 0.1)
lasso.fit(x_train, y_train)

# Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`
y_pred = lasso.predict(x_test)

# Find the R-squared score and store the value in `ridge_score`
lasso_score = r2_score(y_test, y_pred)
lasso_score_rmse = np.sqrt(mean_squared_error(y_test, y_pred))

#score(accuracy)
train_acc_ls =lasso.score(x_train,y_train)
test_acc_ls =lasso.score(x_test,y_test)

print(f'R2 score: {lasso_score}.')
print(f'RMSE score: {lasso_score_rmse}.')
print(f'Training Accuracy: {train_acc_ls}.')
print(f'Test Accuracy: {test_acc_ls}.')

"""####Cross-Validation

Select optimal alpha value using cross-validation.
"""

from sklearn.linear_model import LassoCV

#Lasso Cross validation
lasso_cv = LassoCV(alphas = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5], random_state=0).fit(x_train, y_train)
# Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`
y_pred = lasso_cv.predict(x_test)

# Find the R-squared score and store the value in `ridge_score`
lasso_score = r2_score(y_test, y_pred)
lasso_score_rmse = np.sqrt(mean_squared_error(y_test, y_pred))

#score(accuracy)
train_acc_ls =lasso.score(x_train,y_train)
test_acc_ls =lasso.score(x_test,y_test)

print(f'R2 score: {lasso_score}.')
print(f'RMSE score: {lasso_score_rmse}.')
print(f'Training Accuracy: {train_acc_ls}.')
print(f'Test Accuracy: {test_acc_ls}.')

"""###Ridge Regularized Linear Regression"""

from sklearn.linear_model import Ridge
# Initialize model and fit it
reg_ridge = Ridge(alpha=100)
reg_ridge.fit(x_train, y_train)
# Use the model to predict on the test set and save these predictions as `y_pred`
y_pred = reg_ridge.predict(x_test)
# Find the R-squared score and store the value in `ridge_score`
ridge_score = r2_score(y_test, y_pred)
ridge_score_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
#score(accuracy)
train_acc_rdg =reg_ridge.score(x_train,y_train)
test_acc_rdg =reg_ridge.score(x_test,y_test)

print(f'R2 score: {ridge_score}.')
print(f'RMSE score: {ridge_score_rmse}.')
print(f'Training Accuracy: {train_acc_rdg}.')
print(f'Test Accuracy: {test_acc_rdg}.')

"""###Elastic Net Regularized Linear Regression"""

from sklearn.linear_model import ElasticNet
# Initialize model and fit it on the training set
en = ElasticNet(alpha=2.5, l1_ratio=0.1)
en.fit(x_train, y_train)
# Use the model to predict on the test set and save these predictions as `y_pred`
y_pred = en.predict(x_test)
# Find the R-squared score and store the value in `ridge_score`
en_score = r2_score(y_test, y_pred)
en_score_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
#score(accuracy)
train_acc_en =en.score(x_train,y_train)
test_acc_en =en.score(x_test,y_test)

print(f'R2 score: {en_score}.')
print(f'RMSE score: {en_score_rmse}.')
print(f'Training Accuracy: {train_acc_en}.')
print(f'Test Accuracy: {test_acc_en}.')

"""#**Conclusions**

##Preprocessing

We have a huge and diverse training set but with a large number of missing values for most columns. When we do value imputation, we could try to fill the same value such as zero or mean value into each null, for a given feature. We also try to drop some outliers in the data when it comes to columns related to energy, fat and sugars because unsupervised learning methods such as K-means clustering are very sensitive to outliers.

##EDA

Through our visualizations, we could see how different features are related to each other and how different products are represented by these features. As topics extended from this part, we then analyzed interrelations between selected features using different visualizations and statistical methods.

The geography-based section of our EDA breifly discussed the number of products in different countries. We can see that most products in the dataset are from the US and France. The differences are quite significant, so we can focus on one major country, namely United States, in some of our sections.

We then examined product names for all categories and some particular categories for comparison. Based on previous parts of EDA, we selected plant-based foods and sugary foods as our special focus for not only this section but also later sections of supervised learning.

##Modeling

We divided our learning models into two major types, unsupervised learning and supervised learning, for neatness and organization.

###Unsupervised Learning

We've covered three methods of clustering in this project: k-means, hierarchical clustering, and gaussian mixture models. In the process, we observe that k-means is highly subject to influence of outliers despite we've already dropped outliers! We still have several data cases with extreme salt value in this case. While we are highly susceptible about these data, we are not provided with enough information to rule out these outliers. Nonetheless, we do observe their messy perfomance in k-means. 

In the second clustering method, we choose 1000 sample data to run for the sake of running time and ram. We did agglomerative hierchical clustering and choose cluster number according to the cluster line which has maximal depth that does not cross other cluster lines in the dendogram. Though derive a different cluster number than k-means, the performance is good.

In the third clustering method, which is gaussian mixture model. This model clearly provided a enlighted version of k-means, as it does better on outlier clusters which only had one data in k-means. With the six wordclouds generated, we can roughly interpret each cluster's significations, which is awesome.

###Supervised Learning

The first part of supervised learning is classifications. We conducted binary classifications as well as multi-classification.

We first implemented a binary classification to predict whether a food product is plant-based or not given its contents. We initially applied Logistic Regression model and achieved not too bad but also not satisfying performance, with accuracy less than 75%. We applied PCA but got very little improvement. Then we tried the Decision Tree and Random Forest models, which delivered better accuracies. For these two models, we mainly tried to solve the overfitting problem. We limited their depth and managed to reduce overfitting successfully. To find optimal parameters, we used a loop to keep records of performance for different depth values and plot accuracies against the parameter to identify a suitable value. Eventually, we achieved 83% accuracy with little or no overfitting.

For binary classification in terms of whether or not a product is sugary, we are given a nutrition dataset and want to build a decision tree classifier in order to classify whether a product has more sugar or not. After that, we also use random forest and NN to improve performance. Random forest gives the best result of 94% accuracy on test data. In random forest model, when looking at feature importance, we can see that saturated fat has one of the highest predicting power. It is interesting because this may imply that sugar products are not that healthy. Saturated fat is a type of dietary fat. It is one of the unhealthy fats, along with trans fat. Foods like butter, palm and coconut oils, cheese, and red meat have high amounts of saturated fat. After the models, we use oversampling to solve the problem of data imbalance in sugar classification problem. We achieve balance across classes within our training set. We have a large number of trainning examples that fall into non-sugar class and not the other. The classifer can default to predicting the more frequent of the outcomes. After the oversampling, the random forest model still has the best performance of 94% accuracy.

After binary classifications, we predicted the energy of a food product according to its contents using multi-classification. In this part of learning, we chose Decision Tree and Random Forest as models, and tried to improve their performances by limiting their maximum depths. In the end, we managed to achieve 96% test accuracy. We speculated that this high accuracy might be due to the fact that energy of foods is largely and directly determined by some contents such as fat and carbs, so it can be simple to predict.

In the last section, we predicted the Nutrition Score of a food product. We picked Linear Regression models as it suits preditions on this kind of targets with continuous values. We tried LASSO, Ridge, and Elastic Net Regularizations not only to improve the performance of our model but also to better understand these models by comparisons.

##**Potential Next Steps**

The dataset contains multiple languages, among which English and French accounts for the majority. Due to the limit of GPU and for a acceptable speed of computation, we mainly focused on English entries while doing test-relavant analysis. For a potential future step, translations can be done on category and product_name columns for better wordclouds and further test analysis.

We could try to find extra data about the reviews of the products mentioned in the open food fact dataset and see if we could strengthen our regression model by combining the results of sentiment analysis. But this requires specific reviews on the products about nutrition more than just information about appearance or taste.

For both unsupervised and supervised learning models, there might be more ways of improvements. Also, there are more models that can be used on this dataset and they might have great performances.
"""